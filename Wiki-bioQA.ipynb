{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Looking in indexes: https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/simple\n",
      "Collecting datasets\n",
      "  Downloading https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/packages/packages/9a/a3/9383f255185d7795abdc9a0bde25f8468938ac58594384dde997bdf5e4a9/datasets-1.12.1-py3-none-any.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/packages/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 604 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.0.1)\n",
      "Collecting xxhash\n",
      "  Downloading https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/packages/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\n",
      "Collecting huggingface-hub<0.1.0,>=0.0.14\n",
      "  Downloading https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/packages/packages/ab/45/2d908576740ee8876438cfa2a57aefefeb9f677a3df826d6b4d3c7e6b3cc/huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 190 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
      "Collecting packaging\n",
      "  Downloading https://artifactory.alight.com/artifactory/api/pypi/python-pypi-remote/packages/packages/3c/77/e2362b676dc5008d81be423070dd9577fa03be5da2ba1105811900fda546/packaging-21.0-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 883 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: tqdm, packaging, xxhash, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n",
      "Successfully installed datasets-1.12.1 huggingface-hub-0.0.17 packaging-21.0 tqdm-4.62.3 xxhash-2.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !{sys.executable} -m pip install numpy\n",
    "!pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# import sys\n",
    "# !conda install faiss-cpu==1.7.0 -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset('wiki_bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/wiki_bio/wiki_bio.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2553d6ba2b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wiki_bio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mscript_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m     )\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, script_version, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mreturn_associated_base_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     )\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mprepare_module\u001b[0;34m(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, return_associated_base_path, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_github_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscript_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                     \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mscript_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0muse_etag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_etag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't find file at {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0m_raise_if_offline_mode_is_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tried to reach {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't reach {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;31m# Try a second time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/wiki_bio/wiki_bio.py"
     ]
    }
   ],
   "source": [
    "data = load_dataset('wiki_bio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'target_text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "#list of datasets in pretty-print format\n",
    "pprint(data, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the data and Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (C:\\Users\\a1052739\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9)\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (C:\\Users\\a1052739\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9)\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (C:\\Users\\a1052739\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9)\n"
     ]
    }
   ],
   "source": [
    "## Separating the test, train, and validation sets\n",
    "\n",
    "wiki_train = load_dataset('wiki_bio', split='train')\n",
    "wiki_valid = load_dataset('wiki_bio', split='val')\n",
    "wiki_test = load_dataset('wiki_bio', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': {'context': Value(dtype='string', id=None),\n",
      "                'table': Sequence(feature={'column_header': Value(dtype='string', id=None), 'row_number': Value(dtype='int16', id=None), 'content': Value(dtype='string', id=None)}, length=-1, id=None)},\n",
      " 'target_text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "pprint(wiki_train.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:  ['input_text', 'target_text']\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names: \", wiki_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  582659\n",
      "Number of columns:  2\n",
      "Shape:  (582659, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: \", wiki_train.num_rows)\n",
    "print(\"Number of columns: \", wiki_train.num_columns)\n",
    "print(\"Shape: \", wiki_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First example from the dataset: \n",
      "\n",
      "{'input_text': {'context': 'walter extra\\n',\n",
      "                'table': {'column_header': ['occupation',\n",
      "                                            'name',\n",
      "                                            'nationality',\n",
      "                                            'article_title',\n",
      "                                            'birth_date'],\n",
      "                          'content': ['aircraft designer and manufacturer',\n",
      "                                      'walter extra',\n",
      "                                      'german',\n",
      "                                      'walter extra\\n',\n",
      "                                      '1954'],\n",
      "                          'row_number': [1, 1, 1, 1, 1]}},\n",
      " 'target_text': 'walter extra is a german award-winning aerobatic pilot , '\n",
      "                'chief aircraft designer and founder of extra flugzeugbau '\n",
      "                '-lrb- extra aircraft construction -rrb- , a manufacturer of '\n",
      "                'aerobatic aircraft .\\n'\n",
      "                'extra was trained as a mechanical engineer .\\n'\n",
      "                'he began his flight training in gliders , transitioning to '\n",
      "                'powered aircraft to perform aerobatics .\\n'\n",
      "                'he built and flew a pitts special aircraft and later built '\n",
      "                'his own extra ea-230 .\\n'\n",
      "                'extra began designing aircraft after competing in the 1982 '\n",
      "                'world aerobatic championships .\\n'\n",
      "                'his aircraft constructions revolutionized the aerobatics '\n",
      "                'flying scene and still dominate world competitions .\\n'\n",
      "                'the german pilot klaus schrodt won his world championship '\n",
      "                'title flying an aircraft made by the extra firm .\\n'\n",
      "                'walter extra has designed a series of performance aircraft '\n",
      "                'which include unlimited aerobatic aircraft and turboprop '\n",
      "                'transports .\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(\"First example from the dataset: \\n\")\n",
    "pprint(wiki_train[0])\n",
    "# ['target_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to CSV through Pandas and PyArrow\n",
    " * Not sure if Nessessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyarrow import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wiki_train.to_pandas()\n",
    "test = wiki_test.to_pandas()\n",
    "val = wiki_valid.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'table': {'column_header': ['occupation', 'na...</td>\n",
       "      <td>walter extra is a german award-winning aerobat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'table': {'column_header': ['height', 'caps',...</td>\n",
       "      <td>aaron hohlbein -lrb- born august 16 , 1985 in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'table': {'column_header': ['occupation', 'na...</td>\n",
       "      <td>majda vrhovnik -lrb- nom de guerre lojzka -rrb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'table': {'column_header': ['occupation', 'bi...</td>\n",
       "      <td>linda hayden -lrb- born 19 january 1953 -rrb- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'table': {'column_header': ['originalteam', '...</td>\n",
       "      <td>craig starcevich -lrb- born 16 may 1967 -rrb- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582654</th>\n",
       "      <td>{'table': {'column_header': ['occupation', 'na...</td>\n",
       "      <td>vince mendoza -lrb- born november 17 , 1961 -r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582655</th>\n",
       "      <td>{'table': {'column_header': ['pseudonym', 'occ...</td>\n",
       "      <td>eddy c. bertin is a belgian author of adult an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582656</th>\n",
       "      <td>{'table': {'column_header': ['occupation', 'na...</td>\n",
       "      <td>, is a japanese composer who has worked on sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582657</th>\n",
       "      <td>{'table': {'column_header': ['birth_date', 're...</td>\n",
       "      <td>john strzemp , ii -lrb- born 1952 -rrb- is a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582658</th>\n",
       "      <td>{'table': {'column_header': ['profession', 'te...</td>\n",
       "      <td>jennifer bertino-tarrant is a member of the il...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582659 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_text  \\\n",
       "0       {'table': {'column_header': ['occupation', 'na...   \n",
       "1       {'table': {'column_header': ['height', 'caps',...   \n",
       "2       {'table': {'column_header': ['occupation', 'na...   \n",
       "3       {'table': {'column_header': ['occupation', 'bi...   \n",
       "4       {'table': {'column_header': ['originalteam', '...   \n",
       "...                                                   ...   \n",
       "582654  {'table': {'column_header': ['occupation', 'na...   \n",
       "582655  {'table': {'column_header': ['pseudonym', 'occ...   \n",
       "582656  {'table': {'column_header': ['occupation', 'na...   \n",
       "582657  {'table': {'column_header': ['birth_date', 're...   \n",
       "582658  {'table': {'column_header': ['profession', 'te...   \n",
       "\n",
       "                                              target_text  \n",
       "0       walter extra is a german award-winning aerobat...  \n",
       "1       aaron hohlbein -lrb- born august 16 , 1985 in ...  \n",
       "2       majda vrhovnik -lrb- nom de guerre lojzka -rrb...  \n",
       "3       linda hayden -lrb- born 19 january 1953 -rrb- ...  \n",
       "4       craig starcevich -lrb- born 16 may 1967 -rrb- ...  \n",
       "...                                                   ...  \n",
       "582654  vince mendoza -lrb- born november 17 , 1961 -r...  \n",
       "582655  eddy c. bertin is a belgian author of adult an...  \n",
       "582656  , is a japanese composer who has worked on sev...  \n",
       "582657  john strzemp , ii -lrb- born 1952 -rrb- is a c...  \n",
       "582658  jennifer bertino-tarrant is a member of the il...  \n",
       "\n",
       "[582659 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         walter extra is a german award-winning aerobat...\n",
       "1         aaron hohlbein -lrb- born august 16 , 1985 in ...\n",
       "2         majda vrhovnik -lrb- nom de guerre lojzka -rrb...\n",
       "3         linda hayden -lrb- born 19 january 1953 -rrb- ...\n",
       "4         craig starcevich -lrb- born 16 may 1967 -rrb- ...\n",
       "                                ...                        \n",
       "582654    vince mendoza -lrb- born november 17 , 1961 -r...\n",
       "582655    eddy c. bertin is a belgian author of adult an...\n",
       "582656    , is a japanese composer who has worked on sev...\n",
       "582657    john strzemp , ii -lrb- born 1952 -rrb- is a c...\n",
       "582658    jennifer bertino-tarrant is a member of the il...\n",
       "Name: target_text, Length: 582659, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a FAISS index using DPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c19627c36544c588d993d5d8e0778e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_with_embeddings = data.map(\n",
    "    lambda example: {'embeddings': ctx_encoder(**ctx_tokenizer(example[\"target_text\"][:512], return_tensors=\"pt\"))[0][0].numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _swigfaiss: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\faiss\\loader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading faiss.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mswigfaiss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__package__\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _swigfaiss: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2205d0770e71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import faiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mds_with_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_faiss_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'embeddings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# ds_with_embeddings.is_index_initialized('embeddings')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36madd_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[0;32m   3059\u001b[0m         \"\"\"\n\u001b[0;32m   3060\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatted_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"numpy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3061\u001b[1;33m             super().add_faiss_index(\n\u001b[0m\u001b[0;32m   3062\u001b[0m                 \u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3063\u001b[0m                 \u001b[0mindex_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\search.py\u001b[0m in \u001b[0;36madd_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_factory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstring_factory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         )\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[0mfaiss_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfaiss_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfaiss_verbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\search.py\u001b[0m in \u001b[0;36madd_vectors\u001b[1;34m(self, vectors, column, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mare\u001b[0m \u001b[0minside\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcertain\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mspecify\u001b[0m \u001b[0mit\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# Create index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# We import * so that the symbol foo can be accessed as faiss.foo.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\faiss\\loader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# we import * so that the symbol X can be accessed as faiss.X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading faiss.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mswigfaiss\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Import the low-level C/C++ module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__package__\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _swigfaiss: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "# import faiss\n",
    "# ds_with_embeddings.add_faiss_index(column='embeddings')  \n",
    "# ds_with_embeddings.is_index_initialized('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
